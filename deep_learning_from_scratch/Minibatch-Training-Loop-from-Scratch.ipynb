{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the model from pytorch as we already created a fully connected NN from scratch\n",
    "# We will implement classes for Dataset, a Dataloader and Optimizer from scratch\n",
    "# And finish the notebook with a fully repeatable training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a dataset to train\n",
    "from fastai import datasets\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'\n",
    "\n",
    "def get_data():\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(torch.tensor, (x_train,y_train,x_valid,y_valid))\n",
    "\n",
    "x_train,y_train,x_valid,y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]),\n",
       " torch.Size([50000]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=200, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [nn.Linear(784,200), nn.ReLU(), nn.Linear(200,10)]\n",
    "\n",
    "model = nn.Sequential(*layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50])\n",
      "torch.Size([16, 50])\n",
      "torch.Size([16, 50])\n",
      "torch.Size([16, 50])\n",
      "torch.Size([16, 50])\n",
      "torch.Size([16, 50])\n",
      "torch.Size([16, 50])\n",
      "torch.Size([16, 50])\n",
      "torch.Size([16, 50])\n",
      "torch.Size([16, 50])\n",
      "torch.Size([16, 50])\n",
      "torch.Size([16, 50])\n",
      "torch.Size([8, 50])\n"
     ]
    }
   ],
   "source": [
    "# Minibatch test\n",
    "epochs = 3\n",
    "bs = 16\n",
    "\n",
    "x = torch.randn(200,50)\n",
    "y = torch.randn(200,1)\n",
    "\n",
    "for i in range((len(x)-1)//bs+1):\n",
    "    print(x[i*bs:i*bs+bs].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "bs = 16\n",
    "lr = 0.1\n",
    "\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2529, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#basic training loop with minibatches\n",
    "\n",
    "for i in range(epochs):\n",
    "    for i in range((len(x_train)-1)//bs+1):\n",
    "        \n",
    "        xb = x_train[i*bs:i*bs+bs]\n",
    "        yb = y_train[i*bs:i*bs+bs]\n",
    "    \n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred,yb)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters(): p -= p.grad * lr\n",
    "            model.zero_grad()\n",
    "            \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9570)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(pred, yb): \n",
    "    return (torch.argmax(pred, dim=1)==yb).float().mean()\n",
    "\n",
    "accuracy(model(x_valid), y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Optimizer class\n",
    "\n",
    "class Optimizer():\n",
    "    \"\"\"This class takes care of updating model parameters and\n",
    "    reseting the gradients\"\"\"\n",
    "    \n",
    "    def __init__(self, parameters, lr=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.parameters(): p -= p.grad * lr\n",
    "            model.zero_grad()\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(784,200), nn.ReLU(), nn.Linear(200,10)]\n",
    "\n",
    "model = nn.Sequential(*layers)\n",
    "opt = Optimizer(model.parameters, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1725, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9589)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basic training loop with minibatches\n",
    "\n",
    "for i in range(epochs):\n",
    "    for i in range((len(x_train)-1)//bs+1):\n",
    "        \n",
    "        xb = x_train[i*bs:i*bs+bs]\n",
    "        yb = y_train[i*bs:i*bs+bs]\n",
    "    \n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred,yb)\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "            \n",
    "print(loss)\n",
    "accuracy(model(x_valid), y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    \"\"\"This class holds the dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, x,y):\n",
    "        self.x, self.y = x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "            return self.x[i], self.y[i]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = Dataset(x_train,y_train)\n",
    "\n",
    "print(len(ds_train))\n",
    "\n",
    "x,y = ds_train[10]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader():\n",
    "    \"\"\"This class supports loading batches from the whole dataset one by one.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, bs=16):\n",
    "        self.dataset = dataset\n",
    "        self.bs = bs\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range((len(self.dataset)-1)//self.bs+1):\n",
    "            print(i)\n",
    "            n_start = i * self.bs\n",
    "            n_end = i * self.bs + self.bs\n",
    "            yield self.dataset[n_start:n_end]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) torch.Size([16, 784]) tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "def print_batch():\n",
    "    dl_train = Dataloader(ds_train, 16)\n",
    "\n",
    "    batch = iter(dl_train)\n",
    "    \n",
    "    for i in range(1):\n",
    "        x,y = next(batch)\n",
    "        print(x, x.shape, y, y.shape)\n",
    "\n",
    "print_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refator __iter__ for loop\n",
    "\n",
    "class Dataloader():\n",
    "    \"\"\"This class supports loading batches from the whole dataset one by one.\n",
    "    It also allows for memory optimization for large datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, bs=16):\n",
    "        self.dataset = dataset\n",
    "        self.bs = bs\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.bs):\n",
    "            yield self.dataset[i:i+self.bs]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) torch.Size([16, 784]) tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0822, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9674)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update training loop \n",
    "dl_train = Dataloader(ds_train, 16)\n",
    "\n",
    "for i in range(epochs):\n",
    "    for xb, yb in dl_train:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred,yb)\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "            \n",
    "print(loss)\n",
    "accuracy(model(x_valid), y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([7, 6, 1, 2, 9]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(list(range(100)))\n",
    "\n",
    "torch.randperm(100)[:5]\n",
    "\n",
    "ds_train[torch.randperm(100)[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    \"\"\"Shuffles the dataset for every epoch.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, bs):\n",
    "        self.n = len(dataset)\n",
    "        self.bs = bs\n",
    "        \n",
    "    def __iter__(self):\n",
    "        shuffled_n = torch.randperm(self.n)\n",
    "        \n",
    "        for i in range(0, self.n, self.bs):\n",
    "            yield shuffled_n[i:i+self.bs]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12607, 14062, 17362, 11569, 17652, 29281, 30774, 31564,  4993,  2927,\n",
      "        14066, 37823, 44265,  1977, 35454, 23178])\n",
      "tensor([33139,  5762, 32364, 17667, 24168, 39245, 38993, 37226, 41782, 24556,\n",
      "        44577, 36693, 32249, 42138, 39617, 28805])\n"
     ]
    }
   ],
   "source": [
    "sampler = Sampler(ds_train, 16)\n",
    "\n",
    "batch = iter(sampler)\n",
    "print(next(batch))\n",
    "print(next(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Sampler to Dataloader\n",
    "\n",
    "class Dataloader():\n",
    "    \"\"\"This class supports loading batches from the whole dataset one by one.\n",
    "    It also allows for memory optimization for large datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, bs=16, sampler=Sampler, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.bs = bs\n",
    "        self.sampler = sampler(dataset, bs)\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.shuffle == True:\n",
    "            for sample in self.sampler:\n",
    "                yield self.dataset[sample]\n",
    "        else:\n",
    "            for i in range(0, len(self.dataset), self.bs):\n",
    "                yield self.dataset[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) torch.Size([16, 784]) tensor([4, 6, 1, 5, 8, 7, 9, 1, 8, 4, 1, 5, 1, 2, 6, 4]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0095, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9737)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rerun training loop \n",
    "dl_train = Dataloader(ds_train, 16)\n",
    "\n",
    "for i in range(epochs):\n",
    "    for xb, yb in dl_train:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred,yb)\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "            \n",
    "print(loss)\n",
    "accuracy(model(x_valid), y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration a Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):    \n",
    "    xs, ys = zip(*b)\n",
    "    return torch.stack(xs), torch.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 784]), torch.Size([16]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_train = Dataloader(ds_train, 16)\n",
    "xb,yb = next(iter(dl_train))\n",
    "\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Collate to Dataloader\n",
    "\n",
    "class Dataloader():\n",
    "    \"\"\"This class supports loading batches from the whole dataset one by one.\n",
    "    It also allows for memory optimization for large datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, bs=16, sampler=Sampler, shuffle=True, collate_fn=collate):\n",
    "        self.dataset = dataset\n",
    "        self.bs = bs\n",
    "        self.sampler = sampler(dataset, bs)\n",
    "        self.shuffle = shuffle\n",
    "        self.collate_fn = collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.shuffle == True:\n",
    "            for sample in self.sampler:\n",
    "                yield self.collate_fn([self.dataset[i] for i in sample])\n",
    "        else:\n",
    "            for i in range(0, len(self.dataset), self.bs):\n",
    "                yield self.dataset[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) torch.Size([16, 784]) tensor([7, 0, 7, 7, 8, 6, 7, 3, 3, 5, 9, 2, 2, 6, 8, 0]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0242, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9777)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rerun training loop \n",
    "dl_train = Dataloader(ds_train, 16)\n",
    "\n",
    "for i in range(epochs):\n",
    "    for xb, yb in dl_train:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred,yb)\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "            \n",
    "print(loss)\n",
    "accuracy(model(x_valid), y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
